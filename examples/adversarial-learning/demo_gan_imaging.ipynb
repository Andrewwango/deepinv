{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imaging inverse problems with adversarial networks\n",
    "\n",
    "This example shows you how to train various networks using adversarial training for deblurring problems. We demonstrate running training and inference using DeblurGAN, AmbientGAN and UAIR implemented in the `deepinv` library, and how to simply train your own GAN by using `deepinv.training.AdversarialTrainer`. These examples can also be easily extended to train more complicated GANs such as CycleGAN.\n",
    "\n",
    "- Kupyn et al., [_DeblurGAN: Blind Motion Deblurring Using Conditional Adversarial Networks_](https://openaccess.thecvf.com/content_cvpr_2018/papers/Kupyn_DeblurGAN_Blind_Motion_CVPR_2018_paper.pdf)\n",
    "- Bora et al., [_AmbientGAN: Generative models from lossy measurements_](https://openreview.net/forum?id=Hy7fDog0b)\n",
    "- Pajot et al., [_Unsupervised Adversarial Image Reconstruction_](https://openreview.net/forum?id=BJg4Z3RqF7)\n",
    "\n",
    "Adversarial networks are characterised by the addition of an adversarial loss $\\mathcal{L}_\\text{adv}$ to the standard reconstruction loss:\n",
    "\n",
    "$$\\mathcal{L}_\\text{adv}(x,\\hat x;D)=\\mathbb{E}_{x\\sim p_x}\\left[q(D(x))\\right]+\\mathbb{E}_{\\hat x\\sim p_{\\hat x}}\\left[q(1-D(\\hat x))\\right]$$\n",
    "\n",
    "where $D(\\cdot)$ is the discriminator model, $x$ is the reference image, $\\hat x$ is the estimated reconstruction, $q(\\cdot)$ is a quality function (e.g $q(x)=x$ for WGAN). Training alternates between generator $f$ and discriminator $D$ in a minimax game. When there are no ground truths (i.e unsupervised), this may be defined on the measurements $y$ instead.\n",
    "\n",
    "**DeblurGAN** forward pass: \n",
    "\n",
    "$$\\hat x = f(y)$$\n",
    "\n",
    "**DeblurGAN** loss: \n",
    "\n",
    "$$\\mathcal{L}=\\mathcal{L}_\\text{sup}(\\hat x, x)+\\mathcal{L}_\\text{adv}(\\hat x, x;D)$$\n",
    "\n",
    "where $\\mathcal{L}_\\text{sup}$ is a supervised loss such as pixel-wise MSE or VGG Perceptual Loss.\n",
    "\n",
    "**AmbientGAN** forward pass: \n",
    "\n",
    "$$\\hat x = f(z),\\quad z\\sim \\mathcal{N}(\\mathbf{0},\\mathbf{I}_k)$$\n",
    "\n",
    "**AmbientGAN** loss (where $A(\\cdot)$ is the physics): \n",
    "\n",
    "$$\\mathcal{L}=\\mathcal{L}_\\text{adv}(A(\\hat x), y;D)$$\n",
    "\n",
    "Forward pass at eval time:\n",
    "\n",
    "$$\\hat x = f(\\hat z)\\quad\\text{s.t.}\\quad\\hat z=\\operatorname*{argmin}_z D(A(f(z)),y)$$\n",
    "\n",
    "**UAIR** forward pass:\n",
    "\n",
    "$$\\hat x = f(y)$$\n",
    "\n",
    "**UAIR** loss: \n",
    "\n",
    "$$\\mathcal{L}=\\mathcal{L}_\\text{adv}(\\hat y, y;D)+\\lVert A(f(\\hat y))- \\hat y\\rVert^2_2,\\quad\\hat y=A(\\hat x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepinv as dinv\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor([1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
